#!/usr/local/bin/python
# Caution this pulls real malware samples but renames them to a non exe extension.
# Description: Scrape malc0debd.com for latest malware URLs. Download malware into malware folder to test security stack.

import urllib2, urllib, uuid, time, threading
from BeautifulSoup import BeautifulSoup
from urlparse import urlparse
import os, errno

# Initialize variables
# Uncomment the following line to pull from malc0de.com. When commented this python script will not pull any malware.
#content_url = "http://malc0de.com/database/?&page="
n=1
file_name = 'malcode_urls.txt'
output_dir = "malware"

# Create a file
handle = open(file_name, 'w')

def write_file(text):
	handle.write(text.encode('utf8')+'\n')

def get_page(n):
	req = urllib2.Request(content_url+str(n))
	req.add_header('Referer', 'http://malc0de.com')
	req.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.108 Safari/537.36')
	web_page = urllib2.urlopen(req).read()
	return web_page

def process_page(web_page):
	soup = BeautifulSoup(web_page)
	[s.extract() for s in soup('br')]
	table = soup.find('table', attrs={'class': 'prettytable'})
	rows = table.findAll('tr')
	if len(rows) == 1:
		return 'end'
	for tr in rows:
		text = ''
		n1 = 0
		cols = tr.findAll('td')
		for td in cols:
			n1 += 1
			if n1==2:
				text += 'http://' + "".join(td.contents)
		if text != '':
			write_file(text)
	return 'continue'

# Retrieve malc0de domains
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
x = 1
while x < 5:
	process_page(get_page(x))
	print "Page: " + str(x)
	x += 1
	
# Close file for write
handle.close()

def download_file(val,sema):
	random = output_dir + "/" + str(uuid.uuid4())
	random += urlparse(val).path.split('/')[-1]
	download_file = urllib.URLopener()
	#Timeout(60)
	try:
		download_file.retrieve(str(val), random)
	except:
		print "Failed: " + str(val)
	sema.release()

sema = threading.BoundedSemaphore(value=10)
threads = []
  
with open(file_name) as f:
	content = f.readlines()
	for val in content:
		sema.acquire()
		t = threading.Thread(target=download_file, args=(val, sema))
		t.start()
		threads.append(t)
		# print "."

time.sleep(10)
